{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "import re # imported  to use regular expressions\n",
    "import pickle #Pickling is a way to convert a python object (list, dict, etc.) into a character stream\n",
    "import pandas as pd # Python library to deal with sequential and tabular data\n",
    "import numpy as np #numpy library which provides objects for multi-dimensional arrays.\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #natural language toolkit\n",
    "from nltk.corpus import stopwords #removing stopwords with NLTK in python\n",
    "from nltk.stem import WordNetLemmatizer #to apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.tree import DecisionTreeClassifier #for decision tree classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,average_precision_score,recall_score #Import scikit-learn metrics module for evaluation metrics calculation\n",
    "from sklearn.model_selection import train_test_split #splitting data arrays into two subsets: for training data and for testing data\n",
    "from sklearn import model_selection, naive_bayes, svm #naive bayes and svm algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier #knn algorithm\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier #random forest algorithm\n",
    "from sklearn.linear_model import SGDClassifier# SGD algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file which contains 3130 dataset as a data frame\n",
    "df= pd.read_csv('Dataset_3130_after_webscraping.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data frame into a list \n",
    "\n",
    "C = (df['Content']).tolist()\n",
    "\n",
    "L = (df['Label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING\n",
    "def preprocess(col):\n",
    "    col_l = []\n",
    "    for r in col:\n",
    "        #print(r)\n",
    "        try:\n",
    "            a = r.lower() # returns a copy of the string in which all case-based characters have been lowercased.\n",
    "            a = a.replace(\"'s\", \"\")\n",
    "            #a = a.replace([\"\\n\"], \"hjfgjh\")\n",
    "            #regular expression operations, specify a regular expression pattern in the first argument, a   new string in the second argument, and a string to be processed in the third argument\n",
    "            a = re.sub('[^a-zA-Z0-9 \\n\\.]', '',a)\n",
    "            a = re.sub('[:.\\n\\t]', '',a)\n",
    "            a = re.sub('[?:!.+,]]][]]//;]', '',a) \n",
    "            \n",
    "            col_l.append(a)\n",
    "        except:\n",
    "            #print(\"No header found\")\n",
    "            col_l.append(\"No Header\")\n",
    "            pass\n",
    "\n",
    "    # Downloading punkt and wordnet from NLTK\n",
    "    nltk.download('punkt')\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    nltk.download('wordnet')# used to find the meaning of words\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    nltk.download('stopwords')#removing useless data(the,a,an,in)\n",
    "    \n",
    "    lemmatized_text_list = []\n",
    "\n",
    "    for s in col_l:\n",
    "        #print(s)\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "\n",
    "        # Save the text and its words into an object\n",
    "        #text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = s.split(\" \")\n",
    "\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))#considering only verb\n",
    "\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    new_lemmatized_text_list = []\n",
    "    for x in lemmatized_text_list:\n",
    "        new_lemmatized_text_list.append(\" \".join(x.split())) \n",
    "    \n",
    "    pl = pd.Series(new_lemmatized_text_list)\n",
    "   # pl = pd.Series(col_l)\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    \n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        #pl = pd.Series(col_l)\n",
    "   \n",
    "        pl = pl.str.replace(regex_stopword, '')\n",
    "        \n",
    "    \n",
    "    return pl.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jojy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jojy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jojy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#apply preprocessing for the content C\n",
    "con = preprocess(C)\n",
    "#print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " 'iot-project',\n",
       " ...]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []#initializing the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#’iot-project’ replacing with 1 and ‘non-iot-project’ replacing with 0\n",
    "for i in L:\n",
    "    if i == 'iot-project' or i == 'Iot-Project':\n",
    "        label.append(1)\n",
    "        \n",
    "    elif i == 'non-iot-project':\n",
    "        label.append(0)\n",
    " \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "3125    0\n",
       "3126    0\n",
       "3127    0\n",
       "3128    0\n",
       "3129    0\n",
       "Length: 3130, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la2 = pd.Series(label) #it contains the label and it is considering as y\n",
    "la2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.DataFrame({\"Content\":con})# content is saved in dataframe and it is considering as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connect  modbus energy meter   arduino  monito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remote control  laser   use  multiple argons r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know   go check   trek get call without stall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple tutorial  connect microbit  azure  les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tire  joystick control   phone use l298n ardui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>website collect personal data  use cookies  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>website collect personal data  use cookies  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>reduce data   data center    mentality   past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>four fundamental device management requirement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>requirements   secure internet  things system ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3130 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content\n",
       "0     connect  modbus energy meter   arduino  monito...\n",
       "1     remote control  laser   use  multiple argons r...\n",
       "2     know   go check   trek get call without stall ...\n",
       "3      simple tutorial  connect microbit  azure  les...\n",
       "4     tire  joystick control   phone use l298n ardui...\n",
       "...                                                 ...\n",
       "3125   website collect personal data  use cookies  i...\n",
       "3126   website collect personal data  use cookies  i...\n",
       "3127  reduce data   data center    mentality   past ...\n",
       "3128  four fundamental device management requirement...\n",
       "3129  requirements   secure internet  things system ...\n",
       "\n",
       "[3130 rows x 1 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################  Training Module Starts  #########################################\n",
    "\n",
    "#X value is content and y value is label .we are spliting 15%(470) of dataset as testing and 85% (2660)data is for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, la2, test_size=0.15, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>want  make  goodlooking nonpermanent way  mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>hi guy   go  publish  instructables    long ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>add  twinkle   holiday stock   lilypad lilytin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>grow backyard blackberry  raspberries     alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>dont   love  freshness  spring  instructable  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>model  base   contemporary   time plan   ship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>youre like   wish  enjoy  morning coffee  rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>day  run   coffee roasters  walk around town...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>autonomous water system base  wemos d1 mini co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>ive always marvel   charm  take something ordi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content\n",
       "2133   want  make  goodlooking nonpermanent way  mar...\n",
       "2541  hi guy   go  publish  instructables    long ti...\n",
       "1974  add  twinkle   holiday stock   lilypad lilytin...\n",
       "1496   grow backyard blackberry  raspberries     alw...\n",
       "2085  dont   love  freshness  spring  instructable  ...\n",
       "...                                                 ...\n",
       "1405   model  base   contemporary   time plan   ship...\n",
       "1524   youre like   wish  enjoy  morning coffee  rel...\n",
       "2171    day  run   coffee roasters  walk around town...\n",
       "1035  autonomous water system base  wemos d1 mini co...\n",
       "2343  ive always marvel   charm  take something ordi...\n",
       "\n",
       "[470 rows x 1 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541     1\n",
       "148     1\n",
       "798     1\n",
       "1088    1\n",
       "1854    0\n",
       "       ..\n",
       "2181    0\n",
       "2409    0\n",
       "2033    0\n",
       "1364    1\n",
       "451     1\n",
       "Length: 2660, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters for Tfidf vectorization:\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features =300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vectorization: Transforms text to feature vectors\n",
    "tf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=False, max_df=1.0, max_features=300,\n",
      "                min_df=10, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "tf_transformer = tf.fit(X_train['Content'])\n",
    "print(tf_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the file\n",
    "pickle.dump(tf_transformer, open(\"tfidf1.pkl\", \"wb\"))\n",
    "#load the file\n",
    "tf1 = pickle.load(open(\"tfidf1.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.12915052 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.11788557 0.         0.        ]\n",
      " [0.         0.         0.13334129 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.12018515 0.13452943 0.         ... 0.         0.         0.        ]\n",
      " [0.12565716 0.08307283 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.11049435 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#converting the trained content to vector format\n",
    "features_train= tf1.transform(X_train['Content']).toarray()\n",
    "print(features_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.09351063 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.06079141 0.        ]\n",
      " ...\n",
      " [0.         0.         0.10314554 ... 0.         0.06930043 0.        ]\n",
      " [0.         0.         0.         ... 0.06930961 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#converting the testing content to vector format\n",
    "features_test = tf1.transform(X_test['Content']).toarray()\n",
    "print(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####################################  Training Module Ends  #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## DECISION TREE  Classifier###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=30, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=20, splitter='best')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DTC =DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,max_features=None, max_leaf_nodes=10, min_samples_leaf=5,min_samples_split=2, min_weight_fraction_leaf=0.0,random_state=None, splitter='random')\n",
    "DTC = DecisionTreeClassifier(random_state=20,min_samples_leaf=30)\n",
    "DTC.fit(features_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(DTC, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuacy 0.9984962406015038\n",
      "Testing Accuacy 0.9957446808510638\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result1 = loaded_model.score(features_train, y_train)\n",
    "result = loaded_model.score(features_test, y_test)\n",
    "\n",
    "print(\"Training Accuacy\",result1)\n",
    "print(\"Testing Accuacy\",result)\n",
    "#=loaded_model.predict_proba(features_test[:5])\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting the testing dataset(470)\n",
    "DTC_pred = loaded_model.predict(features_test)\n",
    "DTC_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270   0]\n",
      " [  2 198]]\n"
     ]
    }
   ],
   "source": [
    "#displaying the confusion_matrix\n",
    "matrix = confusion_matrix(y_test, DTC_pred)\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       270\n",
      "           1       1.00      0.99      0.99       200\n",
      "\n",
      "    accuracy                           1.00       470\n",
      "   macro avg       1.00      0.99      1.00       470\n",
      "weighted avg       1.00      1.00      1.00       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying precision,recall and f1 score\n",
    "report = classification_report(y_test, DTC_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############NAIVE BAYES######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Naive = naive_bayes.MultinomialNB(alpha=0.0001)\n",
    "Naive.fit(features_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_naive = 'finalized_naive.sav'\n",
    "pickle.dump(Naive, open(filename_naive, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuacy 0.925187969924812\n",
      "Testing Accuacy 0.9212765957446809\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_naive = pickle.load(open(filename_naive, 'rb'))\n",
    "result = loaded_naive.score(features_train, y_train)\n",
    "result1 = loaded_naive.score(features_test, y_test)\n",
    "print(\"Training Accuacy\",result)\n",
    "print(\"Testing Accuacy\",result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Naive_pred = loaded_naive.predict(features_test)\n",
    "Naive_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[234  36]\n",
      " [  1 199]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       270\n",
      "           1       0.85      0.99      0.91       200\n",
      "\n",
      "    accuracy                           0.92       470\n",
      "   macro avg       0.92      0.93      0.92       470\n",
      "weighted avg       0.93      0.92      0.92       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, Naive_pred)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, Naive_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.SVC(C=0.1,probability=True,kernel='linear')\n",
    "SVM.fit(features_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_svm = 'finalized_svm.sav'\n",
    "pickle.dump(SVM, open(filename_svm, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuacy 0.9917293233082707\n",
      "Testing Accuacy 0.9808510638297873\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_svm = pickle.load(open(filename_svm, 'rb'))\n",
    "result = loaded_svm.score(features_train, y_train)\n",
    "result1 = loaded_svm.score(features_test, y_test)\n",
    "print(\"Training Accuacy\",result)\n",
    "print(\"Testing Accuacy\",result1)\n",
    "#y=loaded_svm.predict_proba(features_test)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pred = loaded_svm.predict(features_test)\n",
    "svm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[261   9]\n",
      " [  0 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       270\n",
      "           1       0.96      1.00      0.98       200\n",
      "\n",
      "    accuracy                           0.98       470\n",
      "   macro avg       0.98      0.98      0.98       470\n",
      "weighted avg       0.98      0.98      0.98       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, svm_pred)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, svm_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7,weights='distance')\n",
    "knn.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_knn = 'finalized_knn.sav'\n",
    "pickle.dump(knn, open(filename_knn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  Accuacy 1.0\n",
      "Testing Accuacy 0.9808510638297873\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_knn = pickle.load(open(filename_knn, 'rb'))\n",
    "result = loaded_knn.score(features_train, y_train)\n",
    "result1= loaded_knn.score(features_test, y_test)\n",
    "print(\"Training  Accuacy\",result)\n",
    "print(\"Testing Accuacy\",result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pred = loaded_knn.predict(features_test)\n",
    "knn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270   0]\n",
      " [  9 191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       270\n",
      "           1       1.00      0.95      0.98       200\n",
      "\n",
      "    accuracy                           0.98       470\n",
      "   macro avg       0.98      0.98      0.98       470\n",
      "weighted avg       0.98      0.98      0.98       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, knn_pred)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, knn_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=8, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=100)\n",
    "SGD = SGDClassifier(random_state=8,loss=\"log\")\n",
    "SGD.fit(features_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_sgd = 'finalized_sgd.sav'\n",
    "pickle.dump(SGD, open(filename_sgd, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuacy 0.9984962406015038\n",
      "Testing Accuacy 0.9914893617021276\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_sgd = pickle.load(open(filename_sgd, 'rb'))\n",
    "result1 = loaded_sgd.score(features_train, y_train)\n",
    "result = loaded_sgd.score(features_test, y_test)\n",
    "print(\"Training Accuacy\",result1)\n",
    "print(\"Testing Accuacy\",result)\n",
    "#y=loaded_sgd.predict_proba(features_test[:5])\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pred = loaded_sgd.predict(features_test)\n",
    "sgd_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[268   2]\n",
      " [  2 198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       270\n",
      "           1       0.99      0.99      0.99       200\n",
      "\n",
      "    accuracy                           0.99       470\n",
      "   macro avg       0.99      0.99      0.99       470\n",
      "weighted avg       0.99      0.99      0.99       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, sgd_pred)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, sgd_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=20, max_features='log2',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=20, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=2,\n",
       "                       n_jobs=None, oob_score=False, random_state=47, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "random = RandomForestClassifier(n_estimators=2,max_features='log2',random_state=47,max_depth=20,min_samples_leaf=20)\n",
    "#random = RandomForestClassifier(n_estimators=20)\n",
    "#regressor = RandomForestRegressor(n_estimators=2, max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "random.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_random = 'finalized_random.sav'\n",
    "pickle.dump(random, open(filename_random, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuacy 0.9924812030075187\n",
      "Testing Accuacy 0.9829787234042553\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_random = pickle.load(open(filename_random, 'rb'))\n",
    "result1 = loaded_random.score(features_train, y_train)\n",
    "result = loaded_random.score(features_test, y_test)\n",
    "print(\"Training Accuacy\",result1)\n",
    "print(\"Testing Accuacy\",result)\n",
    "#y=loaded_random.predict_proba(features_test)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pred = loaded_random.predict(features_test)\n",
    "random_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[268   2]\n",
      " [  2 198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       270\n",
      "           1       0.99      0.99      0.99       200\n",
      "\n",
      "    accuracy                           0.99       470\n",
      "   macro avg       0.99      0.99      0.99       470\n",
      "weighted avg       0.99      0.99      0.99       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, sgd_pred)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, sgd_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936170212765958"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample code for ensambling\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model = VotingClassifier(estimators=[('lr', knn), ('dt', SVM),('sg', SGD)], voting='hard')\n",
    "model.fit(features_train, y_train)\n",
    "model.score(features_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
